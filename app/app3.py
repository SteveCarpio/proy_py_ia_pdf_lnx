import streamlit as st
import time
import ollama
from pathlib import Path

def main():
    # Configuraci√≥n de la p√°gina
    # st.set_page_config(
    #    page_title="Chat con Modelos Locales",
    #    page_icon="ü§ñ",
    #    layout="wide"
    # )

    # Inicializar historial de chat si no existe
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    # Inicializar contador de chats si no existe
    if "chat_counter" not in st.session_state:
        st.session_state.chat_counter = 1
    
    # Sidebar para configuraci√≥n
    with st.sidebar:
        st.markdown("---")
        
        # Bot√≥n para nuevo chat
        if st.button("üÜï Nuevo chat"):
            st.session_state.messages = []
            st.session_state.chat_counter += 1
            st.rerun()
        
        #st.markdown("---")
        
        # Selector de modelo
        model_choice = st.selectbox(
            "üß† Selecciona el modelo:",
            ["llama3:instruct", "mistral:latest"],   # "mixtral:lastest"
            index=0,
            help="Modelos disponibles localmente (en continuo desarrollo)."
        )
        
        #st.markdown("---")
        st.markdown("""
        **üìù Instrucciones:**
        1. Selecciona el modelo
        2. Escribe tu pregunta/prompt
        3. Presiona 'Enviar'
        """)
        
        # Mostrar informaci√≥n del modelo seleccionado
        st.markdown(f"**Modelo activo:** `{model_choice}`")
        st.markdown(f"**Chat actual:** #{st.session_state.chat_counter}")

    # √Årea principal del chat
    st.title(f"ü§ñ ChatTDA (Chat #{st.session_state.chat_counter})")
    st.caption("Nota: Se ejecutar√° usando un modelo pre-entrenado (en construcci√≥n), los datos no se almacenan en ning√∫n servidor exterior/local.")

    # Mostrar historial de chat existente
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # Entrada de usuario
    if prompt := st.chat_input("Escribe tu pregunta aqu√≠..."):
        # Mostrar mensaje del usuario
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # A√±adir al historial
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        # Mostrar respuesta del asistente
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            
            # Generar respuesta
            start_time = time.time()
            
            try:
                # Stream de respuesta desde Ollama usando TODO el historial
                response = ollama.chat(
                    model=model_choice,
                    messages=st.session_state.messages,  # Env√≠a todo el historial
                    stream=True
                )
                
                # Mostrar respuesta en tiempo real
                for chunk in response:
                    chunk_content = chunk.get('message', {}).get('content', '')
                    full_response += chunk_content
                    message_placeholder.markdown(full_response + "‚ñå")
                
                message_placeholder.markdown(full_response)
                
                # Mostrar estad√≠sticas
                end_time = time.time()
                st.caption(f"Tiempo de respuesta: {end_time - start_time:.2f} segundos")
                
                # A√±adir al historial
                st.session_state.messages.append({"role": "assistant", "content": full_response})
                
            except Exception as e:
                st.error(f"Error al generar respuesta: {str(e)}")
                st.session_state.messages.append({"role": "assistant", "content": f"Error: {str(e)}"})

if __name__ == "__main__":
    main()
