import streamlit as st
import time
import ollama
from pathlib import Path

def main():
    # Configuraci√≥n de la p√°gina (debe ser lo primero en Streamlit)
    st.set_page_config(
        page_title="Chat con Modelos Locales",
        page_icon="ü§ñ",
        layout="wide"
    )

    # Sidebar para configuraci√≥n
    with st.sidebar:
        st.title("‚öôÔ∏è Configuraci√≥n del Modelo")
        
        # Selector de modelo
        model_choice = st.selectbox(
            "Selecciona el modelo:",
            ["llama3:instruct", "mistral:latest", "mixtral:latest"],
            index=0,
            help="Modelos disponibles localmente a trav√©s de Ollama"
        )
        
        st.markdown("---")
        st.markdown("""
        **üìù Instrucciones:**
        1. Selecciona el modelo
        2. Escribe tu pregunta/prompt
        3. Presiona 'Enviar'
        """)
        
        # Mostrar informaci√≥n del modelo seleccionado
        st.markdown(f"**Modelo activo:** `{model_choice}`")

    # √Årea principal del chat
    st.title("ü§ñ Chat con Modelos Locales")
    st.caption("Nota: Los modelos se ejecutan localmente en tu servidor Ubuntu a trav√©s de Ollama")

    # Inicializar historial de chat
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Mostrar historial de chat existente
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # Entrada de usuario
    if prompt := st.chat_input("Escribe tu pregunta aqu√≠..."):
        # Mostrar mensaje del usuario
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # A√±adir al historial
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        # Mostrar respuesta del asistente
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            
            # Generar respuesta
            start_time = time.time()
            
            try:
                # Stream de respuesta desde Ollama
                response = ollama.chat(
                    model=model_choice,
                    messages=[{"role": "user", "content": prompt}],
                    stream=True
                )
                
                # Mostrar respuesta en tiempo real
                for chunk in response:
                    chunk_content = chunk.get('message', {}).get('content', '')
                    full_response += chunk_content
                    message_placeholder.markdown(full_response + "‚ñå")
                
                message_placeholder.markdown(full_response)
                
                # Mostrar estad√≠sticas
                end_time = time.time()
                st.caption(f"Tiempo de respuesta: {end_time - start_time:.2f} segundos")
                
                # A√±adir al historial
                st.session_state.messages.append({"role": "assistant", "content": full_response})
                
            except Exception as e:
                st.error(f"Error al generar respuesta: {str(e)}")
                st.session_state.messages.append({"role": "assistant", "content": f"Error: {str(e)}"})

#if __name__ == "__main__":
#    main()